---
title: SAP BTP - Node.js Apllication를 Kyma에 배포하기
published: true
---

SAP BTP Cockpit에 접속하여 하위 계정으로 들어간다.
하위 계정의 하위 도메인을 복사한다.
![image](https://github.com/BJSNuruhee/levelup/assets/88364980/7b6bcf41-8c31-43d8-a948-9a9c9769acd9)

Kyma Cluster domain을 확인해야 한다.
이전에 받았던 kubeconfig.yml에서 확인한다.

![image](https://github.com/BJSNuruhee/levelup/assets/88364980/bdf3b793-7e2b-4e7e-bb22-e6bc5976410f)

//https://api.ccc3b58.kyma.ondemand.com

전 단계에서 어플리케이션을 Kyma 환경에 배포하기 위해선 OCI image를 만들어서 배포해야 한다.
해당 이미지는 pack tool을 이용하여 만든다.

Chocolatey 를 사용하여 설치할건데 설치가 안된 사람들은 구글링하여 찾아보세요..

```
choco install pack --version=0.33.2
```

![image](https://github.com/BJSNuruhee/levelup/assets/88364980/c5a6fe7c-d507-4ef0-9358-b7c1ee7768f6)


packtool을 사용하여 OCI image를 빌드해 봅시다
```
pack build <dockerhub 계정>/multitenant-kyma-backend:v1 --builder paketobuildpacks/builder-jammy-base
```

빌드가 성공하면 이미지를 확인한다. (물론 도커가 설치가 되어있어야 합니다.)
```
docker images
```

![image](https://github.com/BJSNuruhee/levelup/assets/88364980/684a51a2-45f8-424e-9ccb-1a2bcadf0e2e)

docker에 로그인 하고 hub에 push해보자

이 때 로그인 할 때 계정은 이메일 이지만 @ 앞에만 써도 로그인이 되더랍..

```
docker login -u nuruhee -p 패스워드
```

이후 docker hub에 push 해보자
```
docker push nuruhee/multitenant-kyma-backend:v1
```

![image](https://github.com/BJSNuruhee/levelup/assets/88364980/2771de8b-972f-453a-ace0-11c79eb5be66)

![image](https://github.com/BJSNuruhee/levelup/assets/88364980/bd79d1b0-f41a-4a46-b1b2-71fcb276a9b9)

Docker hub에 배포된 이미지를 Kyma 환경에서 가져와보자

하위계정에서 대시보드에 링크 클릭
![image](https://github.com/BJSNuruhee/levelup/assets/88364980/3360e378-ae8a-46d9-aae4-e5e3321b5059)

NameSpace를 만들어주고
![image](https://github.com/BJSNuruhee/levelup/assets/88364980/abc57004-2291-4376-bdbc-258ddc7dddb7)

Kyma가 Docker Hub에서 가져올 수 있도록 액세스 정보 설정
```
kubectl -n nuruhee create secret docker-registry registry-secret --docker-server=https://index.docker.io/v1/  --docker-username=nuruhee --docker-password=패스워드 --docker-email=nuruhee@gmail.com
```

명령어를 통해 액세스 정보를 설정하려고 하니 실패하였다.
```
error: failed to create secret Post "http://localhost:8080/api/v1/namespaces/nuruhee/secrets?fieldManager=kubectl-create&fieldValidation=Strict": dial tcp [::1]:8080: connectex: No connection could be made because the target machine actively refused it.
```

명령어를 통해 환경변수를 확인해보았다.

```
kubectl config view
```

```
apiVersion: v1
clusters: null
contexts: null
current-context: ""
kind: Config
preferences: {}
users: null
```

아무것도 적용된 환경변수가 없어서 환경변수로 kubeconfig를 따로 잡았다.
(이전에 명령어로 kubenconfig를 환경변수로 잡았지만 cmd창을 재기동하게 되면 환경변수가 초기화되는 현상이 있어서 아래 방법으로 진행하는 걸 권장한다.)
![image](https://github.com/BJSNuruhee/levelup/assets/88364980/b815ed5f-6a00-4e1d-9f17-78b2c333ce4b)

다시 명령어를 쳐서 실행해보니 registry-secret이 생성되었다.
```
kubectl -n nuruhee create secret docker-registry registry-secret --docker-server=https://index.docker.io/v1/  --docker-username=nuruhee --docker-password=패스워드 --docker-email=nuruhee@gmail.com
```
![image](https://github.com/BJSNuruhee/levelup/assets/88364980/e6145da0-fbb0-4f1d-bafc-3753aeaf556c)

하위계정 -> Kyma Dashboard -> Configuration -> Secrets 에  만든 registry-secret이 생겼다.
![image](https://github.com/BJSNuruhee/levelup/assets/88364980/da247588-9493-42b5-b50c-fdf47f878d68)

요 과정은  Kubernetes 클러스터에서 특정 네임스페이스에 Docker 레지스트리 인증 정보를 저장하기 위한 시크릿을 생성한 것이다.

그렇다면 이제 Kubernetes 클러스터의 특정 네임스페이스에서 Docker 로그인 정보를 가진 상태가 된 것이다.

배포 전에 요약 파일을 yaml 파일로 만들어서 배포한다.
k8s-deployment-backend.yml 파일을 만들고 안에 내용을 채워준다.
```
---
apiVersion: gateway.kyma-project.io/v1beta1
kind: APIRule
metadata:
  labels:
    app: kyma-multitenant-node-multitenancy
    release: multitenancy
  name: kyma-multitenant-node-multitenancy
spec:
  gateway: kyma-gateway.kyma-system.svc.cluster.local
  host: kyma-multitenant-node-multitenancy
  rules:
  - accessStrategies:
    - handler: allow
    methods:
    - GET
    - POST
    - PUT
    - PATCH
    - DELETE
    - HEAD
    path: /.*
  service:
    name: kyma-multitenant-node-multitenancy
    port: 8080

---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: kyma-multitenant-node-multitenancy
    release: multitenancy
  name: kyma-multitenant-node-multitenancy
spec:
  replicas: 1
  selector:
    matchLabels:
      app: kyma-multitenant-node-multitenancy
      release: multitenancy
  template:
    metadata:
      labels:
        app: kyma-multitenant-node-multitenancy
        release: multitenancy
    spec:
      imagePullSecrets:
        - name: registry-secret # replace with your own registry secret
      containers:
      - env:
        - name: PORT
          value: "8080"
        - name: TMPDIR
          value: /tmp
        image: <docker-hub-account>/multitenant-kyma-backend:v1  # replace with your Docker Hub account name
        name: kyma-multitenant-node-multitenancy
        ports:
        - name: http
          containerPort: 8080
          protocol: TCP
        livenessProbe:
          httpGet:
            path: /
            port: http
        readinessProbe:
          httpGet:
            path: /
            port: http
        startupProbe:
          httpGet:
            path: /
            port: http
          failureThreshold: 15
          periodSeconds: 2
        resources:
          limits:
            cpu: 100m
            memory: 256M
          requests:
            cpu: 100m
            memory: 256M
        securityContext:
          allowPrivilegeEscalation: false
          capabilities:
            drop:
            - ALL
          privileged: false
          runAsNonRoot: true
          readOnlyRootFilesystem: false
        volumeMounts:
        - mountPath: /tmp
          name: tmp
      volumes:
      - emptyDir: {}
        name: tmp

---
apiVersion: v1
kind: Service
metadata:
  creationTimestamp: null
  labels:
    app: kyma-multitenant-node-multitenancy
    release: multitenancy
  name: kyma-multitenant-node-multitenancy
spec:
  ports:
  - port: 8080
    protocol: TCP
    targetPort: 8080
  selector:
    app: kyma-multitenant-node-multitenancy
    release: multitenancy
status:
  loadBalancer: {}

---
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  creationTimestamp: null
  labels:
    release: multitenancy
  name: multitenancy
spec:
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          name: istio-system
      podSelector:
        matchLabels:
          app: istio-ingressgateway
  podSelector:
    matchLabels:
      release: multitenancy
  policyTypes:
  - Ingress
```

이 때 위의 내용을 그대로 복붙해서 쓰지 말고.. 꼭 설정 파일을 읽고 바꿔야 할 부분을 바꿔서 쓰자 ^^
![image](https://github.com/BJSNuruhee/levelup/assets/88364980/3558c1e7-bfcd-45c3-95a6-cb06f7689e8e)

배포 설명 파일까지 만들었으니 이제 Kyma에 배포를 해보자.
튜토리얼을 만들었던 루트 폴더로 이동해서 명령어 실행
![image](https://github.com/BJSNuruhee/levelup/assets/88364980/8285a19f-58da-4f11-9104-f254832b1da2)

```
kubectl -n 본인이설정해준네임스페이스 apply -f k8s-deployment-backend.yaml
```

![image](https://github.com/BJSNuruhee/levelup/assets/88364980/b5ef9884-7c5f-47dd-904d-ea77c6b00bd5)

도커 이미지를 이용하여 Kyma에 배포했으니 이제 브라우저로 해당 어플리케이션을 접근해보자.

하위 계정 -> Discovery Network -> Host
![image](https://github.com/BJSNuruhee/levelup/assets/88364980/8f353af7-0791-49b7-b6ec-238b9c15a792)


뭔가 제대로 뜨지 않는다.
그래서 여러가지 명령어로 왜 제대로 뜨지 않나 확인을 해보았더니
```
kubectl get pods --all-namespaces
kubectl describe pod kyma-multitenant-node-multitenancy-86555db664-lnwj7 -n nuruhee
```

image를 땡겨 올 때 권한 문제가 걸렸다.

![image](https://github.com/BJSNuruhee/levelup/assets/88364980/3e6683ea-f744-4513-af85-b0c134b1fdfe)

알고보니 액세스 설정을 할 때 
```
kubectl -n nuruhee create secret docker-registry registry-secret --docker-server=https://index.docker.io/v1/  --docker-username=nuruhee --docker-password=패스워드 --docker-email=nuruhee@gmail.com
```

요 부분에서 docker-username 뒤에 이메일을 포함한 아이디가 들어가야 된다는거.. 재배포를 해보니 정상적으로 동작은 하나
![image](https://github.com/BJSNuruhee/levelup/assets/88364980/ac5e8c78-8afc-45f2-9367-512641f0702d)

화면은 제대로 뜨지 않는 현상이 나타남
![image](https://github.com/BJSNuruhee/levelup/assets/88364980/b48d801d-3855-40a1-986d-8d5573cb8492)

.